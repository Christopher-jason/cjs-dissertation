{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft 2 - Dissertation\n",
    "\n",
    "Sentiment analysis to understand the effect of meme community on the stock market. This paper studies how r/wallstreetbets have an impact on the stock market and how some companies are discussed more due to the relevance/popularity of the company in the general space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "\n",
    "TODO: ML libraries to be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import yfinance as yf\n",
    "import openai\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the symbols into a variable \n",
    "\n",
    "and search any specific symbol, and/or add \"$\" to a paticular symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = pd.read_csv('symbols/symbols.csv')\n",
    "symbols.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search for a specific symbol\n",
    "sym = '$NOW'\n",
    "print(symbols.loc[symbols['Symbol'] == sym])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a \"$\" to a specific Symbol column in the symbols df from the change variable matching the Symbols column\n",
    "change = 'NEXT'\n",
    "symbols.loc[symbols['Symbol'] == change, 'Symbol'] = '$' + change\n",
    "print(symbols.loc[symbols['Symbol'] == '$'+change])\n",
    "#save the symbols df in the csv file\n",
    "symbols.to_csv('symbols/symbols.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data - Remove duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, 'r') as f:\n",
    "            sentences = f.readlines()\n",
    "\n",
    "        # Remove duplicates while preserving the order\n",
    "        unique_sentences = list(dict.fromkeys(sentences))\n",
    "\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.writelines(unique_sentences)\n",
    "\n",
    "        print(\"Duplicates removed successfully and saved to\", output_file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the input file path.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File inputs\n",
    "input_file = 'raw_txt/hot.txt'\n",
    "output_file = 'clean_txt/clean_hot.txt'\n",
    "\n",
    "remove_duplicates(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File inputs\n",
    "input_file = 'raw_txt/new.txt'\n",
    "output_file = 'clean_txt/clean_new.txt'\n",
    "\n",
    "remove_duplicates(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the hot file into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the hot text file to a list\n",
    "hot = open('clean_txt/clean_hot.txt').read().split('\\n')\n",
    "#count the number of items in the list\n",
    "len(hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = open('clean_txt/clean_new.txt').read().split('\\n')\n",
    "len(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the lines into a dataframe with dates and text\n",
    "\n",
    "this is done so it is easy to get the dates with the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframe(file_name):\n",
    "    # Initialize a list to store the DataFrame rows\n",
    "    rows = []\n",
    "\n",
    "    # Open the file for reading\n",
    "    with open(file_name, 'r') as file:\n",
    "        current_date = None\n",
    "\n",
    "        for line in file:\n",
    "            # Check if the line matches the date format\n",
    "            match = re.search(r'[-]+(\\d{4}-\\d{2}-\\d{2})', line)\n",
    "            if match:\n",
    "                current_date = match.group(1) # Only extract the date part without dashes\n",
    "            else:\n",
    "                if current_date is not None: # Avoid adding rows without an associated date\n",
    "                    rows.append({'date': current_date, 'text': line.strip()})\n",
    "\n",
    "    # Create a DataFrame from the list of rows\n",
    "    df = pd.DataFrame(rows, columns=['date', 'text'])\n",
    "\n",
    "    # Print the DataFrame to see the results\n",
    "    print(df)\n",
    "\n",
    "    # Return the DataFrame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_df = make_dataframe('clean_txt/clean_hot.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[new_df['text'] == \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = make_dataframe('clean_txt/clean_new.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write hot_df into a csv file named hot_df with no index\n",
    "hot_df.to_csv('text_df/hot_df.csv', index=False)\n",
    "new_df.to_csv('text_df/new_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_df = pd.read_csv('text_df/hot_df.csv')\n",
    "print(f\"Hot length: {len(hot_ticker)}\")\n",
    "new_df = pd.read_csv('text_df/new_df.csv')\n",
    "print(f\"New length: {len(new_ticker)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the tickers in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symbol_matches(sentence, symbol_list):\n",
    "    matches = []\n",
    "    for symbol in symbol_list:\n",
    "        if pd.notna(symbol):  # Skip NaN values in the symbol_list\n",
    "            pattern = r\"\\b\" + re.escape(str(symbol)) + r\"\\b\"\n",
    "            if re.search(pattern, sentence):\n",
    "                matches.append(symbol)\n",
    "    return matches\n",
    "\n",
    "def find_matches(hot_df, symbols):\n",
    "    # List to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in hot_df.iterrows():\n",
    "        date = row['date']\n",
    "        sentence = row['text']\n",
    "\n",
    "        # Find matches for each sentence with \"Symbol\" and \"Dollar_Symbol\" columns\n",
    "        matches_symbols = symbol_matches(sentence, symbols['Symbol'].tolist())\n",
    "        matches_dollar_symbols = symbol_matches(sentence, symbols['Dollar_Symbol'].tolist())\n",
    "        all_matches = matches_symbols + matches_dollar_symbols\n",
    "\n",
    "        # If all_matches is empty then input \"S&P\"\n",
    "        if not all_matches:\n",
    "            all_matches = ['S&P']\n",
    "\n",
    "        # Concatenate the matches into a single string\n",
    "        ticker_string = ', '.join(all_matches)\n",
    "\n",
    "        # Append the results to the list\n",
    "        results.append((date, sentence, ticker_string))\n",
    "\n",
    "    # Create a new DataFrame to store the results\n",
    "    tickers = pd.DataFrame(results, columns=[\"date\", \"text\", \"ticker\"])\n",
    "\n",
    "    # Print the result DataFrame\n",
    "    print(tickers)\n",
    "\n",
    "    # Return the result DataFrame  \n",
    "    return tickers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_ticker = find_matches(hot_df,symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ticker = find_matches(new_df,symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to exclude rows with 'S&P' in the 'Ticker' column\n",
    "non_sp_sentences = hot_ticker[hot_ticker['ticker'] != 'S&P']\n",
    "len(non_sp_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_sentences = hot_ticker[hot_ticker['ticker'] == 'S&P']\n",
    "len(sp_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write hot_ticker to a csv with no index\n",
    "hot_ticker.to_csv('text_df/hot_ticker.csv', index=False)\n",
    "new_ticker.to_csv('text_df/new_ticker.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read hot_ticker csv to a variable name hot_ticker\n",
    "hot_ticker = pd.read_csv('text_df/hot_ticker.csv')\n",
    "print(f\"Hot length: {len(hot_ticker)}\")\n",
    "new_ticker = pd.read_csv('text_df/new_ticker.csv')\n",
    "print(f\"New length: {len(new_ticker)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe_finbert = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert_sentiments(df,pipe=pipe_finbert):\n",
    "    # Create an empty list to store the rows\n",
    "    rows = []\n",
    "    \n",
    "    # Iterate through the DataFrame rows\n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "\n",
    "        # If text is a string, convert it to a list with one element\n",
    "        if isinstance(text, str):\n",
    "            sentences = [text]\n",
    "        \n",
    "        # Process the sentences through the pipeline\n",
    "        results = pipe(sentences)\n",
    "        \n",
    "        # Iterate through the sentences and results to create rows\n",
    "        for sentence, result in zip(sentences, results):\n",
    "            rows.append((date, sentence, ticker, result['label'], result['score']))\n",
    "            \n",
    "    # Create a DataFrame from the rows\n",
    "    finbert_sentiment = pd.DataFrame(rows, columns=['date', 'text', 'ticker', 'label', 'score'])\n",
    "    \n",
    "    return finbert_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_finbert = finbert_sentiments(hot_ticker)\n",
    "print(hot_finbert.head(5))\n",
    "print(f\"Hot Finbert length: {len(hot_ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_finbert = finbert_sentiments(new_ticker)\n",
    "print(new_finbert.head(5))\n",
    "print(f\"New Finbert length: {len(new_ticker)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write hot_sentiments into a csv with no index\n",
    "hot_finbert.to_csv('sentiment_scores/hot_finbert.csv', index=False)\n",
    "new_finbert.to_csv('sentiment_scores/new_finbert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_finbert[hot_finbert['ticker'].str.contains(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_finbert[new_finbert['ticker'].str.contains(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer_vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_sentiments(df,analyzer=analyzer_vader):\n",
    "    # Create an empty list to store the rows\n",
    "    rows = []\n",
    "    \n",
    "    # Iterate through the keys and values in hot_dict\n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "        # If sentences is a string, convert it to a list with one element\n",
    "        if isinstance(text, str):\n",
    "            sentences = [text]\n",
    "        \n",
    "        # Process the sentences through the pipeline\n",
    "        for sentence in sentences:\n",
    "            result = analyzer.polarity_scores(sentence)\n",
    "            rows.append((date, sentence, ticker, result['neg'], result['neu'], result['pos'], result['compound']))\n",
    "            \n",
    "    # Create a DataFrame from the rows\n",
    "    vader_sentiment = pd.DataFrame(rows, columns=['date', 'text', 'ticker', 'neg', 'neu', 'pos', 'compound'])\n",
    "    \n",
    "    return vader_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_vader = vader_sentiments(hot_ticker)\n",
    "print(hot_vader.head(10))\n",
    "print(f\"Lenght of Hot Vader: {len(hot_vader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vader = vader_sentiments(new_ticker)\n",
    "print(new_vader.head(10))\n",
    "print(f\"Lenght of Hot Vader: {len(new_vader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write hot_vader to csv file with no index\n",
    "hot_vader.to_csv('sentiment_scores/hot_vader.csv', index=False)\n",
    "new_vader.to_csv('sentiment_scores/new_vader.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(content):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Analyze the given text and classify it into: negative, or positive. Also provide a sentiment score within the range of -1 to 1. Score values must be calculated with high precision with up to three decimal places. Your response format should be: sentiment, score e.g., ('negative, -0.145').\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "def chatgpt_df(rows):\n",
    "    chatgpt = pd.DataFrame(rows, columns=['date', 'text', 'ticker', 'label', 'score'])\n",
    "    return chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_sentiments(df, tmp_df=tmp_df, max_retries=5, base_delay=1, analyzer=openai):\n",
    "    rows = []\n",
    "    total_tokens = 0\n",
    "    calls_per_minute = 1000\n",
    "    call_interval = 60 / calls_per_minute\n",
    "    last_call_time = 0\n",
    "    last_token_reset_time = time.time() # Keep track of when the token count was last reset\n",
    "    max_loop_count = 10 * max_retries # Maximum number of consecutive loop iterations without progress\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "        retries = 0\n",
    "        loop_count = 0 # Counter for loop iterations without progress\n",
    "        \n",
    "        while retries < max_retries:\n",
    "            current_time = time.time()\n",
    "            time_since_last_call = current_time - last_call_time\n",
    "            time_since_last_token_reset = current_time - last_token_reset_time\n",
    "\n",
    "            if time_since_last_token_reset >= 60:\n",
    "                total_tokens = 0 # Reset token count if it's been more than a minute\n",
    "                last_token_reset_time = current_time\n",
    "\n",
    "            if time_since_last_call < call_interval or total_tokens >= 85000:\n",
    "                sleep_time = call_interval - time_since_last_call\n",
    "                print(f\"Rate limit reached, sleeping for {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "                loop_count += 1\n",
    "                if loop_count > max_retries:  # Break out of the loop if too many iterations have occurred\n",
    "                    print(\"Max loop count reached, unable to process the request.\")\n",
    "                    break\n",
    "                continue \n",
    "                \n",
    "                loop_count = 0\n",
    "            try:\n",
    "                last_call_time = time.time() # Update the time of the last successful call\n",
    "                result = get_response(text)\n",
    "                tokens = result['usage']['total_tokens']\n",
    "                total_tokens += tokens # Update the total token count\n",
    "                content = result.choices[0].message['content']\n",
    "                try:\n",
    "                    label, score_str = content.split(',', 1) # Split only at the first comma\n",
    "                    label = label.strip()\n",
    "                    score = float(score_str.strip())\n",
    "                except ValueError:\n",
    "                    print(f\"Error here: {content}\")\n",
    "                    label = \"error\"\n",
    "                    score = 2\n",
    "                \n",
    "                rows.append((date, text, ticker, label, score))\n",
    "                print(f\"Sentiment captured: {score}, token_used: {tokens}, total for the min: {total_tokens}\")\n",
    "                break\n",
    "\n",
    "            except openai.error.ServiceUnavailableError:\n",
    "                tmp_df = chatgpt_df(rows) #save the answers till the error point.\n",
    "                tmp_df.to_csv('tmp.csv', index=False) #write tmp_df to a csv file named tmp with no index\n",
    "                print(\"Did not get result.\")\n",
    "                delay = base_delay * (2 ** retries)\n",
    "                print(f\"Service unavailable, retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                retries += 1\n",
    "            except Exception as e:  # Catch-all for other exceptions\n",
    "                tmp_df = chatgpt_df(rows) #save the answers till the error point.\n",
    "                tmp_df.to_csv('tmp.csv', index=False)\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                delay = base_delay * (2 ** retries)\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                retries += 1\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Operation was interrupted by the user.\")\n",
    "                tmp_df = chatgpt_df(rows) #save the answers till the error point.\n",
    "                tmp_df.to_csv('tmp.csv', index=False)\n",
    "                return chatgpt_df(rows)\n",
    "        else: # This will execute if the while loop ends without a break statement\n",
    "            print(\"Max retries reached, unable to process the request.\")\n",
    "\n",
    "    return chatgpt_df(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all rows after row 697 in hot_ticker\n",
    "hot_chatgpt = chatgpt_sentiments(hot_ticker)\n",
    "len(hot_chatgpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_chatgpt[hot_chatgpt['label']=='error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = chatgpt_sentiments(hot_chatgpt[hot_chatgpt['label']=='error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(hot_chatgpt, tmp, on='text', suffixes=('_hot', '_tmp'))\n",
    "\n",
    "# Iterate through the merged DataFrame and update the label and score in hot_chatgpt\n",
    "for index, row in merged_df.iterrows():\n",
    "    hot_index = hot_chatgpt[hot_chatgpt['text'] == row['text']].index.item()\n",
    "    hot_chatgpt.at[hot_index, 'label'] = row['label_tmp']\n",
    "    hot_chatgpt.at[hot_index, 'score'] = row['score_tmp']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_chatgpt[hot_chatgpt['label'] == 'error']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove errors from Hot_chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the rows from hot_chatgpt where the label is 'error'\n",
    "hot_chatgpt = hot_chatgpt[hot_chatgpt['label'] != 'error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write hot_chatgpt to hot_chatgpt.csv with no index using pandas\n",
    "hot_chatgpt.to_csv('sentiment_scores/hot_chatgpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the tmp.csv file into a variavle name tmp1\n",
    "hot_chatgpt = pd.read_csv('sentiment_scores/hot_chatgpt.csv')\n",
    "hot_chatgpt.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chatgpt = chatgpt_sentiments(new_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tmp = chatgpt_sentiments(new_chatgpt[new_chatgpt['score']==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merged_df = pd.merge(new_chatgpt, new_tmp, on='text', suffixes=('_new', '_tmp'))\n",
    "\n",
    "# Iterate through the merged DataFrame and update the label and score in hot_chatgpt\n",
    "for index, row in new_merged_df.iterrows():\n",
    "    new_index = new_chatgpt[new_chatgpt['text'] == row['text']].index.item()\n",
    "    new_chatgpt.at[new_index, 'label'] = row['label_tmp']\n",
    "    new_chatgpt.at[new_index, 'score'] = row['score_tmp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chatgpt[new_chatgpt['score']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chatgpt = new_chatgpt[new_chatgpt['score']!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chatgpt.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chatgpt.to_csv('sentiment_scores/new_chatgpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGPT calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Make sure to download the NLTK tokenizer models\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(df):\n",
    "    total = 0\n",
    "    # Iterate through the DataFrame rows\n",
    "    for index, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        \n",
    "        # Tokenize the text using NLTK\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Count the number of tokens\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        total += num_tokens\n",
    "        # Print the number of tokens\n",
    "        print(f\"Number of tokens in row {index}: {num_tokens}\")\n",
    "    print(f\" Total number of tokens {total}\")\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = count_tokens(hot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokensgpt = tokens /1000\n",
    "print(f\" Charged tokens {tokensgpt}\")\n",
    "price_gpt3 = 0.0015\n",
    "print(f\" Price per token for GPT_3 4K {price_gpt3}\")\n",
    "print(f\" Total cost ${tokensgpt * price_gpt3}\")\n",
    "iterations = 50\n",
    "print(f\" Iterations cost ${(tokensgpt * price_gpt3)*iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DISTIL ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe_roberta = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_sentiments(df,pipe=pipe_roberta):\n",
    "    # Create an empty list to store the rows\n",
    "    rows = []\n",
    "    \n",
    "    # Iterate through the DataFrame rows\n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "\n",
    "        # If text is a string, convert it to a list with one element\n",
    "        if isinstance(text, str):\n",
    "            sentences = [text]\n",
    "        \n",
    "        # Process the sentences through the pipeline\n",
    "        results = pipe(sentences)\n",
    "        \n",
    "        # Iterate through the sentences and results to create rows\n",
    "        for sentence, result in zip(sentences, results):\n",
    "            rows.append((date, sentence, ticker, result['label'], result['score']))\n",
    "            \n",
    "    # Create a DataFrame from the rows\n",
    "    roberta_sentiment = pd.DataFrame(rows, columns=['date', 'text', 'ticker', 'label', 'score'])\n",
    "    \n",
    "    return roberta_sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_roberta = roberta_sentiments(hot_ticker)\n",
    "print(hot_roberta.head())\n",
    "new_roberta = roberta_sentiments(new_ticker)\n",
    "print(new_roberta.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write sentiments into a csv with no index\n",
    "hot_roberta.to_csv('sentiment_scores/hot_roberta.csv', index=False)\n",
    "new_roberta.to_csv('sentiment_scores/new_roberta.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price download and calculations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def price_tickers(ticker_df):\n",
    "    # Count the occurrences of each ticker and sort them in descending order\n",
    "    ticker_counts_df = ticker_df['ticker'].value_counts().sort_values(ascending=False).reset_index()\n",
    "\n",
    "    # Reset the index and rename the columns for clarity\n",
    "    ticker_counts_df.columns = ['Ticker', 'Count']\n",
    "    # Filter the tickers that occur more than three times\n",
    "    frequent_tickers_df = ticker_counts_df[ticker_counts_df['Count'] > 3]\n",
    "    print(frequent_tickers_df)\n",
    "    # Convert the filtered tickers to a list\n",
    "    frequent_tickers_list = frequent_tickers_df['Ticker'].tolist()\n",
    "\n",
    "    # Return the list of frequent tickers\n",
    "    return frequent_tickers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_price_ticker = price_tickers(new_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_price_ticker= price_tickers(hot_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_stock_data(ticker):\n",
    "    start_date = pd.to_datetime('2022-08-02')\n",
    "    end_date = pd.to_datetime('2023-08-17')\n",
    "    #download the stock\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_ticker(ticker_list,folder):\n",
    "    for ticker in ticker_list:\n",
    "        print(ticker)\n",
    "        # Call the download_stock_data function to get adjusted closing prices\n",
    "        stock_data = download_stock_data(ticker)\n",
    "        #write the stock data to a csv file in the price_data folder\n",
    "        stock_data.to_csv('price_data/'+folder+'/'+ticker+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_ticker(hot_price_ticker,'hot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_ticker(new_price_ticker,'new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test S&P 500 since the symbol in yf is unique\n",
    "sp = download_stock_data(\"^GSPC\")\n",
    "sp.head()\n",
    "#write it to a csv file\n",
    "sp.to_csv('price_data/hot/S&P.csv')\n",
    "sp.to_csv('price_data/new/S&P.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_price_data(ticker_list,folder):\n",
    "    data_dict = {}\n",
    "    for ticker in ticker_list:\n",
    "        file_path = os.path.join('price_data',f'{folder}' ,f'{ticker}.csv')\n",
    "        if os.path.exists(file_path):\n",
    "            data_dict[ticker] = pd.read_csv(file_path)\n",
    "        else:\n",
    "            print(f\"File for {ticker} does not exist!\")\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_price_data = read_price_data(hot_price_ticker,'hot')\n",
    "new_price_data = read_price_data(new_price_ticker,'new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_price_data['S&P'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_price_data['S&P'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(price_data):\n",
    "    for ticker, df in price_data.items():\n",
    "        close_prices = df['Close'].tolist()\n",
    "        returns = [0]  # Initialize returns with 0 for the first item\n",
    "        for i in range(1, len(close_prices)):\n",
    "            ret = (close_prices[i] - close_prices[i-1]) / close_prices[i-1]\n",
    "            returns.append(ret)\n",
    "        df['Returns'] = returns\n",
    "        price_data[ticker] = df  # Update the dictionary with the modified DataFrame\n",
    "    return price_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_price_data_ret = calculate_returns(hot_price_data)\n",
    "hot_price_data_ret['S&P'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_price_data_ret = calculate_returns(new_price_data)\n",
    "new_price_data_ret['AAPL'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEEDBACK - 1 Week calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weekly_returns(price_data):\n",
    "    for ticker, df in price_data.items():\n",
    "        close_prices = df['Close']\n",
    "        for days in range(1, 6):\n",
    "            df[f'Returns_{days}'] = close_prices.pct_change(periods=days)\n",
    "        price_data[ticker] = df\n",
    "    return price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_price_data_weekly_ret = calculate_weekly_returns(hot_price_data)\n",
    "hot_price_data_weekly_ret['S&P'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_price_data_weekly_ret = calculate_weekly_returns(new_price_data)\n",
    "new_price_data_weekly_ret['S&P'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add returns to the sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the sentiment files\n",
    "hot_finbert = pd.read_csv('sentiment_scores/hot_finbert.csv')\n",
    "hot_vader = pd.read_csv('sentiment_scores/hot_vader.csv')\n",
    "hot_chatgpt = pd.read_csv('sentiment_scores/hot_chatgpt.csv')\n",
    "hot_roberta = pd.read_csv('sentiment_scores/hot_roberta.csv')\n",
    "new_finbert = pd.read_csv('sentiment_scores/new_finbert.csv')\n",
    "new_vader = pd.read_csv('sentiment_scores/new_vader.csv')\n",
    "new_chatgpt = pd.read_csv('sentiment_scores/new_chatgpt.csv')\n",
    "new_roberta = pd.read_csv('sentiment_scores/new_roberta.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_returns(sentiment_df, price_data_ret):\n",
    "    returns = []  # List to store the returns that will be matched\n",
    "\n",
    "    # Iterate through the rows in sentiment_df\n",
    "    for index, row in sentiment_df.iterrows():\n",
    "        ticker = row['ticker']\n",
    "        date = row['date']\n",
    "\n",
    "        # Check if the ticker exists in price_data_ret\n",
    "        if ticker in price_data_ret:\n",
    "            ticker_df = price_data_ret[ticker]\n",
    "        else:\n",
    "            # If ticker not found, use the 'S&P' DataFrame\n",
    "            ticker_df = price_data_ret['S&P']\n",
    "\n",
    "        # Check if the date exists in the DataFrame for the ticker\n",
    "        matching_date = ticker_df[ticker_df['Date'] == date]\n",
    "\n",
    "        # If a matching date is found, get the corresponding return\n",
    "        if not matching_date.empty:\n",
    "            ret = matching_date['Returns'].iloc[0]\n",
    "            returns.append(ret)\n",
    "        else:\n",
    "            returns.append(None)  # If date not found, append None\n",
    "\n",
    "    # Add the returns list as a new column in sentiment_df\n",
    "    sentiment_df['ret'] = returns\n",
    "\n",
    "    return sentiment_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_finbert_ret = merge_returns(hot_finbert,hot_price_data_ret)\n",
    "print(hot_finbert_ret.head())\n",
    "new_finbert_ret = merge_returns(new_finbert,new_price_data_ret)\n",
    "print(new_finbert_ret.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_vader_ret = merge_returns(hot_vader,hot_price_data_ret)\n",
    "print(hot_vader_ret.head())\n",
    "new_vader_ret = merge_returns(new_vader,new_price_data_ret)\n",
    "print(new_vader_ret.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_chatgpt_ret = merge_returns(hot_chatgpt,hot_price_data_ret)\n",
    "print(hot_chatgpt_ret.head())\n",
    "new_chatgpt_ret = merge_returns(new_chatgpt,new_price_data_ret)\n",
    "print(new_chatgpt_ret.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_roberta_ret = merge_returns(hot_roberta,hot_price_data_ret)\n",
    "print(hot_roberta_ret.head())\n",
    "new_roberta_ret = merge_returns(new_roberta,new_price_data_ret)\n",
    "print(new_roberta_ret.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_finbert_ret.to_csv('sentiment_score_ret/hot_finbert_ret.csv', index= False)\n",
    "hot_vader_ret.to_csv('sentiment_score_ret/hot_vader_ret.csv', index= False)\n",
    "hot_chatgpt_ret.to_csv('sentiment_score_ret/hot_chatgpt_ret.csv', index= False)\n",
    "hot_roberta_ret.to_csv('sentiment_score_ret/hot_roberta_ret.csv', index=False)\n",
    "new_finbert_ret.to_csv('sentiment_score_ret/new_finbert_ret.csv', index= False)\n",
    "new_vader_ret.to_csv('sentiment_score_ret/new_vader_ret.csv', index= False)\n",
    "new_chatgpt_ret.to_csv('sentiment_score_ret/new_chatgpt_ret.csv', index= False)\n",
    "new_roberta_ret.to_csv('sentiment_score_ret/new_roberta_ret.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_finbert_ret = pd.read_csv('sentiment_score_ret/hot_finbert_ret.csv')\n",
    "hot_vader_ret = pd.read_csv('sentiment_score_ret/hot_vader_ret.csv')\n",
    "hot_chatgpt_ret = pd.read_csv('sentiment_score_ret/hot_chatgpt_ret.csv')\n",
    "hot_roberta_ret = pd.read_csv('sentiment_score_ret/hot_roberta_ret.csv')\n",
    "new_finbert_ret = pd.read_csv('sentiment_score_ret/new_finbert_ret.csv')\n",
    "new_vader_ret = pd.read_csv('sentiment_score_ret/new_vader_ret.csv')\n",
    "new_chatgpt_ret = pd.read_csv('sentiment_score_ret/new_chatgpt_ret.csv')\n",
    "new_roberta_ret = pd.read_csv('sentiment_score_ret/new_roberta_ret.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_weekly_returns(sentiment_df, price_data_ret):\n",
    "    returns_columns = [f'Returns_{i}' for i in range(1, 6)]\n",
    "    returns_dict = {col: [] for col in returns_columns}  # Dictionary to store the returns for each period\n",
    "\n",
    "    # Iterate through the rows in sentiment_df\n",
    "    for index, row in sentiment_df.iterrows():\n",
    "        ticker = row['ticker']\n",
    "        date = row['date']\n",
    "\n",
    "        # Check if the ticker exists in price_data_ret\n",
    "        ticker_df = price_data_ret.get(ticker, price_data_ret['S&P'])  # Use 'S&P' if ticker not found\n",
    "\n",
    "        # Check if the date exists in the DataFrame for the ticker\n",
    "        matching_date = ticker_df[ticker_df['Date'] == date]\n",
    "\n",
    "        # If a matching date is found, get the corresponding returns for each period\n",
    "        if not matching_date.empty:\n",
    "            for col in returns_columns:\n",
    "                returns_dict[col].append(matching_date[col].iloc[0])\n",
    "        else:\n",
    "            # If date not found, append None for each period\n",
    "            for col in returns_columns:\n",
    "                returns_dict[col].append(None)\n",
    "\n",
    "    # Add the returns lists as new columns in sentiment_df\n",
    "    for col, ret_values in returns_dict.items():\n",
    "        sentiment_df[col] = ret_values\n",
    "\n",
    "    return sentiment_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finbert\n",
    "hot_finbert_weekly_ret = merge_weekly_returns(hot_finbert,hot_price_data_weekly_ret)\n",
    "new_finbert_weekly_ret = merge_weekly_returns(new_finbert,new_price_data_weekly_ret)\n",
    "#vader\n",
    "hot_vader_weekly_ret = merge_weekly_returns(hot_vader,hot_price_data_weekly_ret)\n",
    "new_vader_weekly_ret = merge_weekly_returns(new_vader,new_price_data_weekly_ret)\n",
    "#chatgpt\n",
    "hot_chatgpt_weekly_ret = merge_weekly_returns(hot_chatgpt,hot_price_data_weekly_ret)\n",
    "new_chatgpt_weekly_ret = merge_weekly_returns(new_chatgpt,new_price_data_weekly_ret)\n",
    "#roberta\n",
    "hot_roberta_weekly_ret = merge_weekly_returns(hot_roberta,hot_price_data_weekly_ret)\n",
    "new_roberta_weekly_ret = merge_weekly_returns(new_roberta,new_price_data_weekly_ret)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hot weekly return file\n",
    "hot_finbert_weekly_ret.to_csv('sentiment_score_ret/weekly_data/hot_finbert_weekly_ret.csv', index= False)\n",
    "hot_vader_weekly_ret.to_csv('sentiment_score_ret/weekly_data/hot_vader_weekly_ret.csv', index= False)\n",
    "hot_chatgpt_weekly_ret.to_csv('sentiment_score_ret/weekly_data/hot_chatgpt_weekly_ret.csv', index= False)\n",
    "hot_roberta_weekly_ret.to_csv('sentiment_score_ret/weekly_data/hot_roberta_weekly_ret.csv', index=False)\n",
    "#New weekly return file\n",
    "new_finbert_weekly_ret.to_csv('sentiment_score_ret/weekly_data/new_finbert_weekly_ret.csv', index= False)\n",
    "new_vader_weekly_ret.to_csv('sentiment_score_ret/weekly_data/new_vader_weekly_ret.csv', index= False)\n",
    "new_chatgpt_weekly_ret.to_csv('sentiment_score_ret/weekly_data/new_chatgpt_weekly_ret.csv', index= False)\n",
    "new_roberta_weekly_ret.to_csv('sentiment_score_ret/weekly_data/new_roberta_weekly_ret.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    ax2.plot(history.history['accuracy'])\n",
    "    ax2.plot(history.history['val_accuracy'])\n",
    "    ax2.set_title('Model accuracy')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM(df, max_words=10000):\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    \n",
    "    # Label encoding for 'label' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['label'])\n",
    "    ticker_encoder = LabelEncoder()\n",
    "    df['ticker'] = ticker_encoder.fit_transform(df['ticker'])\n",
    "\n",
    "    df['score'] = df['score'].astype(float)\n",
    "    # Separate features and target variable\n",
    "    X = df[['text', 'label', 'ticker', 'score']]\n",
    "    y = (df['ret'] > 0).astype(int)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenize and pad the 'text' column for training and testing sets\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train['text'])\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train['text'])\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test['text'])\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Text Input (LSTM)\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    text_embedding = Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_lstm = LSTM(128, return_sequences=True)(text_embedding)\n",
    "    text_lstm2 = LSTM(64)(text_lstm) # Additional LSTM layer\n",
    "    text_dense = Dense(32, activation='relu')(text_lstm2)\n",
    "\n",
    "    # Ticker Input \n",
    "    ticker_input = Input(shape=(1,), name='ticker_input')\n",
    "    ticker_dense = Dense(16, activation='relu')(ticker_input)\n",
    "\n",
    "    # Label Input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_dense = Dense(16, activation='relu')(label_input)\n",
    "\n",
    "    # Score Input\n",
    "    score_input = Input(shape=(1,), name='score_input')\n",
    "    score_dense1 = Dense(16, activation='relu')(score_input)\n",
    "    score_dense2 = Dense(8, activation='relu')(score_dense1) # Additional dense layer\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenated = concatenate([text_dense, ticker_dense, label_dense, score_dense2])\n",
    "    dense_1 = Dense(64, activation='relu')(concatenated)\n",
    "    output = Dense(1, activation='sigmoid')(dense_1)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[text_input, ticker_input, label_input, score_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    #Print the model\n",
    "    tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "    #model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train['ticker'].values, 'label_input': X_train['label'].values, 'score_input': X_train['score'].values},\n",
    "        y_train,\n",
    "        epochs=7,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Plot the loss\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test['ticker'].values, 'label_input': X_test['label'].values, 'score_input': X_test['score'].values},\n",
    "        y_test\n",
    "    )\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT FINBERT LSTM  =============== \\n\")\n",
    "model_LSTM(hot_finbert_ret)\n",
    "print(\"\\n\\n=============  NEW FINBERT LSTM  =============== \")\n",
    "model_LSTM(new_finbert_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT CHAT-GPT 3.5 LSTM  =============== \\n\")\n",
    "model_LSTM(hot_chatgpt_ret)\n",
    "print(\"\\n\\n=============  NEW CHAT-GPT 3.5 LSTM  =============== \")\n",
    "model_LSTM(new_chatgpt_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM_vader(df, max_words=10000):\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    # Label encoding for 'ticker' column\n",
    "    ticker_encoder = LabelEncoder()\n",
    "    df['ticker'] = ticker_encoder.fit_transform(df['ticker'])\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = df[['text', 'ticker', 'neg', 'neu', 'pos', 'compound']]\n",
    "    y = (df['ret'] > 0).astype(int)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenize and pad the 'text' column for training and testing sets\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train['text'])\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train['text'])\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test['text'])\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Text Input (LSTM)\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    text_embedding = Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_lstm = LSTM(128, return_sequences=True)(text_embedding)\n",
    "    text_lstm2 = LSTM(64)(text_lstm)\n",
    "    text_dense = Dense(32, activation='relu')(text_lstm2)\n",
    "\n",
    "    # Ticker Input (numeric)\n",
    "    ticker_input = Input(shape=(1,), name='ticker_input')\n",
    "    ticker_dense = Dense(16, activation='relu')(ticker_input)\n",
    "\n",
    "    # Other Numeric Inputs (neg, neu, pos, compound)\n",
    "    other_inputs = Input(shape=(4,), name='other_inputs')  # 4 numeric features\n",
    "    other_dense = Dense(16, activation='relu')(other_inputs)\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenated = concatenate([text_dense, ticker_dense, other_dense])\n",
    "    dense_1 = Dense(64, activation='relu')(concatenated)\n",
    "    output = Dense(1, activation='sigmoid')(dense_1)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[text_input, ticker_input, other_inputs], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train['ticker'].values, 'other_inputs': X_train[['neg', 'neu', 'pos', 'compound']].values},\n",
    "        y_train,\n",
    "        epochs=7,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Plot the loss\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test['ticker'].values, 'other_inputs': X_test[['neg', 'neu', 'pos', 'compound']].values},\n",
    "        y_test\n",
    "    )\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT CHAT-GPT 3.5 LSTM  =============== \\n\")\n",
    "model_LSTM_vader(hot_vader_ret)\n",
    "print(\"\\n\\n=============  NEW CHAT-GPT 3.5 LSTM  =============== \")\n",
    "model_LSTM_vader(new_vader_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT ROBERTA LSTM  =============== \\n\")\n",
    "model_LSTM(hot_roberta_ret)\n",
    "print(\"\\n\\n=============  NEW ROBERTA LSTM  =============== \")\n",
    "model_LSTM(new_roberta_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_CNN(df, max_words=10000):\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    # One-hot encoding for 'label' and 'ticker' columns\n",
    "    one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "    label_one_hot = one_hot_encoder.fit_transform(df[['label']])\n",
    "    ticker_one_hot = one_hot_encoder.fit_transform(df[['ticker']])\n",
    "\n",
    "    df['score'] = df['score'].astype(float)\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X_text = df['text']\n",
    "    X_ticker = ticker_one_hot\n",
    "    X_label = label_one_hot\n",
    "    X_score = df['score'].values.reshape(-1, 1)\n",
    "    y = (df['ret'] > 0).astype(int)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train_text, X_test_text, X_train_ticker, X_test_ticker, X_train_label, X_test_label, X_train_score, X_test_score, y_train, y_test = train_test_split(X_text, X_ticker, X_label, X_score, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenize and pad the 'text' column for training and testing sets\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train_text)\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test_text)\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Text Input (CNN)\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    text_embedding = Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_conv1 = Conv1D(128, 3, activation='relu')(text_embedding)  # Reduced kernel size\n",
    "    text_pool1 = MaxPooling1D(3)(text_conv1)                        # Reduced pooling size\n",
    "    text_conv2 = Conv1D(128, 3, activation='relu')(text_pool1)      # Reduced kernel size\n",
    "    text_pool2 = MaxPooling1D(3)(text_conv2)                        # Reduced pooling size\n",
    "    text_flat = GlobalMaxPooling1D()(text_pool2)\n",
    "    text_dense = Dense(32, activation='relu')(text_flat)\n",
    "    # Ticker Input (numeric)\n",
    "    ticker_input_shape = X_ticker.shape[1]\n",
    "    ticker_input = Input(shape=(ticker_input_shape,), name='ticker_input')\n",
    "\n",
    "    # Label Input (numeric)\n",
    "    label_input_shape = X_label.shape[1]\n",
    "    label_input = Input(shape=(label_input_shape,), name='label_input')\n",
    "\n",
    "    # Score Input (numeric)\n",
    "    score_input = Input(shape=(1,), name='score_input')\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenated = concatenate([text_dense, ticker_input, label_input, score_input])\n",
    "    dense_1 = Dense(64, activation='relu')(concatenated)\n",
    "    output = Dense(1, activation='sigmoid')(dense_1)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[text_input, ticker_input, label_input, score_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train_ticker, 'label_input': X_train_label, 'score_input': X_train_score},\n",
    "        y_train,\n",
    "        epochs=7,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Plot the loss\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test_ticker, 'label_input': X_test_label, 'score_input': X_test_score},\n",
    "        y_test\n",
    "    )\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT FINBERT CNN  =============== \\n\")\n",
    "model_CNN(hot_finbert_ret)\n",
    "print(\"\\n\\n=============  NEW FINERT CNN  =============== \")\n",
    "model_CNN(new_finbert_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT CHAT GPT 3.5 CNN  =============== \\n\")\n",
    "model_CNN(hot_chatgpt_ret)\n",
    "print(\"\\n\\n=============  NEW CHAT GPT 3.5 CNN  =============== \")\n",
    "model_CNN(new_chatgpt_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_CNN_vader(df, max_words=10000):\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    # One-hot encoding for 'ticker' column\n",
    "    one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "    ticker_one_hot = one_hot_encoder.fit_transform(df[['ticker']])\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X_text = df['text']\n",
    "    X_ticker = ticker_one_hot\n",
    "    X_neg = df['neg'].values.reshape(-1, 1)\n",
    "    X_neu = df['neu'].values.reshape(-1, 1)\n",
    "    X_pos = df['pos'].values.reshape(-1, 1)\n",
    "    X_compound = df['compound'].values.reshape(-1, 1)\n",
    "    y = (df['ret'] > 0).astype(int)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    splits = train_test_split(X_text, X_ticker, X_neg, X_neu, X_pos, X_compound, y, test_size=0.2, random_state=42)\n",
    "    X_train_text, X_test_text, X_train_ticker, X_test_ticker, X_train_neg, X_test_neg, X_train_neu, X_test_neu, X_train_pos, X_test_pos, X_train_compound, X_test_compound, y_train, y_test = splits\n",
    "\n",
    "    # Tokenize and pad the 'text' column\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train_text)\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test_text)\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Text Input (CNN)\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    text_embedding = Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_conv1 = Conv1D(128, 3, activation='relu')(text_embedding)\n",
    "    text_pool1 = MaxPooling1D(3)(text_conv1)\n",
    "    text_conv2 = Conv1D(128, 3, activation='relu')(text_pool1)\n",
    "    text_pool2 = MaxPooling1D(3)(text_conv2)\n",
    "    text_flat = GlobalMaxPooling1D()(text_pool2)\n",
    "    text_dense = Dense(32, activation='relu')(text_flat)\n",
    "\n",
    "    # Additional Inputs (numeric)\n",
    "    ticker_input = Input(shape=(X_ticker.shape[1],), name='ticker_input')\n",
    "    neg_input = Input(shape=(1,), name='neg_input')\n",
    "    neu_input = Input(shape=(1,), name='neu_input')\n",
    "    pos_input = Input(shape=(1,), name='pos_input')\n",
    "    compound_input = Input(shape=(1,), name='compound_input')\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenated = concatenate([text_dense, ticker_input, neg_input, neu_input, pos_input, compound_input])\n",
    "    dense_1 = Dense(64, activation='relu')(concatenated)\n",
    "    output = Dense(1, activation='sigmoid')(dense_1)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[text_input, ticker_input, neg_input, neu_input, pos_input, compound_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train_ticker, 'neg_input': X_train_neg, 'neu_input': X_train_neu, 'pos_input': X_train_pos, 'compound_input': X_train_compound},\n",
    "        y_train,\n",
    "        epochs=7,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Plot the loss\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test_ticker, 'neg_input': X_test_neg, 'neu_input': X_test_neu, 'pos_input': X_test_pos, 'compound_input': X_test_compound},\n",
    "        y_test\n",
    "    )\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT VADER CNN  =============== \\n\")\n",
    "model_CNN_vader(hot_vader_ret)\n",
    "print(\"\\n\\n=============  NEW VADER CNN  =============== \")\n",
    "model_CNN_vader(new_vader_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT ROBERTA CNN  =============== \\n\")\n",
    "model_CNN(hot_roberta_ret)\n",
    "print(\"\\n\\n=============  NEW ROBERTA CNN  =============== \")\n",
    "model_CNN(new_roberta_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_CNN_LSTM(df, max_words=10000):\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    # One-hot encoding for 'label' and 'ticker' columns\n",
    "    one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "    label_one_hot = one_hot_encoder.fit_transform(df[['label']])\n",
    "    ticker_one_hot = one_hot_encoder.fit_transform(df[['ticker']])\n",
    "    df['score'] = df['score'].astype(float)\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X_text = df['text']\n",
    "    X_ticker = ticker_one_hot\n",
    "    X_label = label_one_hot\n",
    "    X_score = df['score'].values.reshape(-1, 1)\n",
    "    y = (df['ret'] > 0).astype(int)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train_text, X_test_text, X_train_ticker, X_test_ticker, X_train_label, X_test_label, X_train_score, X_test_score, y_train, y_test = train_test_split(X_text, X_ticker, X_label, X_score, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenize and pad the 'text' column for training and testing sets\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train_text)\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test_text)\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Text Input (CNN followed by LSTM)\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    text_embedding = Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_conv1 = Conv1D(128, 3, activation='relu')(text_embedding)\n",
    "    text_pool1 = MaxPooling1D(3)(text_conv1)\n",
    "    text_conv2 = Conv1D(128, 3, activation='relu')(text_pool1)\n",
    "    text_pool2 = MaxPooling1D(3)(text_conv2)\n",
    "    text_lstm1 = LSTM(128, return_sequences=True)(text_pool2)\n",
    "    text_lstm2 = LSTM(64)(text_lstm1)  # Additional LSTM layer\n",
    "    text_dense = Dense(32, activation='relu')(text_lstm2)\n",
    "\n",
    "    # Ticker Input (numeric)\n",
    "    ticker_input_shape = X_ticker.shape[1]\n",
    "    ticker_input = Input(shape=(ticker_input_shape,), name='ticker_input')\n",
    "    ticker_dense = Dense(16, activation='relu')(ticker_input)\n",
    "\n",
    "    # Label Input (numeric)\n",
    "    label_input_shape = X_label.shape[1]\n",
    "    label_input = Input(shape=(label_input_shape,), name='label_input')\n",
    "    label_dense = Dense(16, activation='relu')(label_input)\n",
    "\n",
    "    # Score Input (numeric)\n",
    "    score_input = Input(shape=(1,), name='score_input')\n",
    "    score_dense1 = Dense(16, activation='relu')(score_input)\n",
    "    score_dense2 = Dense(8, activation='relu')(score_dense1)  # Additional dense layer\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenated = concatenate([text_dense, ticker_dense, label_dense, score_dense2])\n",
    "    dense_1 = Dense(64, activation='relu')(concatenated)\n",
    "    output = Dense(1, activation='sigmoid')(dense_1)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[text_input, ticker_input, label_input, score_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train_ticker, 'label_input': X_train_label, 'score_input': X_train_score},\n",
    "        y_train,\n",
    "        epochs=7,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Plot the loss\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test_ticker, 'label_input': X_test_label, 'score_input': X_test_score},\n",
    "        y_test\n",
    "    )\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT FINBERT CNN-LSTM  =============== \\n\")\n",
    "model_CNN_LSTM(hot_finbert_ret)\n",
    "print(\"\\n\\n=============  NEW FINERT CNN-LSTM   =============== \")\n",
    "model_CNN_LSTM(new_finbert_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT CHAT GPT 3.5 CNN-LSTM  =============== \\n\")\n",
    "model_CNN_LSTM(hot_chatgpt_ret)\n",
    "print(\"\\n\\n=============  NEW CHAT GPT 3.5 CNN-LSTM   =============== \")\n",
    "model_CNN_LSTM(new_chatgpt_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_CNN_LSTM_vader(df, max_words=10000):\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    # One-hot encoding for 'ticker' column\n",
    "    one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "    ticker_one_hot = one_hot_encoder.fit_transform(df[['ticker']])\n",
    "    \n",
    "    # Separate features and target variable\n",
    "    X_text = df['text']\n",
    "    X_ticker = ticker_one_hot\n",
    "    X_neg = df['neg'].values.reshape(-1, 1)\n",
    "    X_neu = df['neu'].values.reshape(-1, 1)\n",
    "    X_pos = df['pos'].values.reshape(-1, 1)\n",
    "    X_compound = df['compound'].values.reshape(-1, 1)\n",
    "    y = (df['ret'] > 0).astype(int)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train_text, X_test_text, X_train_ticker, X_test_ticker, X_train_neg, X_test_neg, X_train_neu, X_test_neu, X_train_pos, X_test_pos, X_train_compound, X_test_compound, y_train, y_test = train_test_split(X_text, X_ticker, X_neg, X_neu, X_pos, X_compound, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenize and pad the 'text' column for training and testing sets\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train_text)\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test_text)\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Text Input (CNN followed by LSTM)\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    text_embedding = Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_conv1 = Conv1D(128, 3, activation='relu')(text_embedding)\n",
    "    text_pool1 = MaxPooling1D(3)(text_conv1)\n",
    "    text_conv2 = Conv1D(128, 3, activation='relu')(text_pool1)\n",
    "    text_pool2 = MaxPooling1D(3)(text_conv2)\n",
    "    text_lstm1 = LSTM(128, return_sequences=True)(text_pool2)\n",
    "    text_lstm2 = LSTM(64)(text_lstm1)  # Additional LSTM layer\n",
    "    text_dense = Dense(32, activation='relu')(text_lstm2)\n",
    "\n",
    "    # Ticker Input (numeric)\n",
    "    ticker_input_shape = X_ticker.shape[1]\n",
    "    ticker_input = Input(shape=(ticker_input_shape,), name='ticker_input')\n",
    "    ticker_dense = Dense(16, activation='relu')(ticker_input)\n",
    "\n",
    "    # Sentiment Scores Input (numeric)\n",
    "    neg_input = Input(shape=(1,), name='neg_input')\n",
    "    neu_input = Input(shape=(1,), name='neu_input')\n",
    "    pos_input = Input(shape=(1,), name='pos_input')\n",
    "    compound_input = Input(shape=(1,), name='compound_input')\n",
    "\n",
    "    # Concatenate sentiment scores\n",
    "    sentiment_scores = concatenate([neg_input, neu_input, pos_input, compound_input])\n",
    "    sentiment_dense = Dense(16, activation='relu')(sentiment_scores)\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenated = concatenate([text_dense, ticker_dense, sentiment_dense])\n",
    "    dense_1 = Dense(64, activation='relu')(concatenated)\n",
    "    output = Dense(1, activation='sigmoid')(dense_1)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[text_input, ticker_input, neg_input, neu_input, pos_input, compound_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Model summary\n",
    "    model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train_ticker, 'neg_input': X_train_neg, 'neu_input': X_train_neu, 'pos_input': X_train_pos, 'compound_input': X_train_compound},\n",
    "        y_train,\n",
    "        epochs=7,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Plot the loss (you'll need to define or import a function to do this)\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test_ticker, 'neg_input': X_test_neg, 'neu_input': X_test_neu, 'pos_input': X_test_pos, 'compound_input': X_test_compound},\n",
    "        y_test\n",
    "    )\n",
    "    print(\"Test Accuracy: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT VADER CNN-LSTM  =============== \\n\")\n",
    "model_CNN_LSTM_vader(hot_vader_ret)\n",
    "print(\"\\n\\n=============  NEW VADER CNN-LSTM   =============== \")\n",
    "model_CNN_LSTM_vader(new_vader_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT ROBERTA CNN_LSTM  =============== \\n\")\n",
    "model_CNN_LSTM(hot_roberta_ret)\n",
    "print(\"\\n\\n=============  NEW ROBERTA CNN_LSTM  =============== \")\n",
    "model_CNN_LSTM(new_roberta_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weekly Price Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_LSTM_weekly(df, return_day=1, max_words=10000):\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    \n",
    "    # Label encoding for 'label' column\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['label'] = label_encoder.fit_transform(df['label'])\n",
    "    ticker_encoder = LabelEncoder()\n",
    "    df['ticker'] = ticker_encoder.fit_transform(df['ticker'])\n",
    "\n",
    "    df['score'] = df['score'].astype(float)\n",
    "    # Separate features and target variable\n",
    "    X = df[['text', 'label', 'ticker', 'score']]\n",
    "    y = (df[f'Returns_{return_day}'] > 0).astype(int)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Tokenize and pad the 'text' column for training and testing sets\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train['text'])\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train['text'])\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test['text'])\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Text Input (LSTM)\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    text_embedding = Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_lstm = LSTM(128, return_sequences=True)(text_embedding)\n",
    "    text_lstm2 = LSTM(64)(text_lstm) # Additional LSTM layer\n",
    "    text_dense = Dense(32, activation='relu')(text_lstm2)\n",
    "\n",
    "    # Ticker Input \n",
    "    ticker_input = Input(shape=(1,), name='ticker_input')\n",
    "    ticker_dense = Dense(16, activation='relu')(ticker_input)\n",
    "\n",
    "    # Label Input\n",
    "    label_input = Input(shape=(1,), name='label_input')\n",
    "    label_dense = Dense(16, activation='relu')(label_input)\n",
    "\n",
    "    # Score Input\n",
    "    score_input = Input(shape=(1,), name='score_input')\n",
    "    score_dense1 = Dense(16, activation='relu')(score_input)\n",
    "    score_dense2 = Dense(8, activation='relu')(score_dense1) # Additional dense layer\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    concatenated = concatenate([text_dense, ticker_dense, label_dense, score_dense2])\n",
    "    dense_1 = Dense(64, activation='relu')(concatenated)\n",
    "    output = Dense(1, activation='sigmoid')(dense_1)\n",
    "\n",
    "    # Build and compile the model\n",
    "    model = Model(inputs=[text_input, ticker_input, label_input, score_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    #Print the model\n",
    "    tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "    #model summary\n",
    "    #model.summary()\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train['ticker'].values, 'label_input': X_train['label'].values, 'score_input': X_train['score'].values},\n",
    "        y_train,\n",
    "        epochs=5,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    # Plot the loss\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    loss, accuracy = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test['ticker'].values, 'label_input': X_test['label'].values, 'score_input': X_test['score'].values},\n",
    "        y_test\n",
    "    )\n",
    "    print(\"Test Accuracy for Returns from day \",return_day,\" : {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINBERT WEEKLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT FINBERT LSTM  =============== \\n\")\n",
    "for i in range(1,6):\n",
    "    model_LSTM_weekly(hot_finbert_weekly_ret,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW FINBERT WEEKLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  NEW FINBERT LSTM  =============== \\n\")\n",
    "for i in range(1,6):\n",
    "    model_LSTM_weekly(new_finbert_weekly_ret,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHATGPT WEEKLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT CHAT GPT 3.5 LSTM  =============== \\n\")\n",
    "for i in range(1,6):\n",
    "    model_LSTM_weekly(hot_chatgpt_weekly_ret,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW CHATGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  NEW CHAT GPT 3.5 LSTM  =============== \\n\")\n",
    "for i in range(1,6):\n",
    "    model_LSTM_weekly(new_chatgpt_weekly_ret,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROBERTA WEEKLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT ROBERTA LSTM  =============== \\n\")\n",
    "for i in range(1,6):\n",
    "    model_LSTM_weekly(hot_roberta_weekly_ret,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW ROBERTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  NEW ROBERTA LSTM  =============== \\n\")\n",
    "for i in range(1,6):\n",
    "    model_LSTM_weekly(new_roberta_weekly_ret,i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
