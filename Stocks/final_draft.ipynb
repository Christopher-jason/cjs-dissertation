{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Project: Impact of r/wallstreetbets analysed using LSTM and CNN\n",
    "\n",
    "Date: 26/8/2023\n",
    "\n",
    "Author: Christopher Jason Sagayaraj\n",
    "\n",
    "Sentiment analysis to understand the effect of meme community on the stock market. \n",
    "This paper studies how r/wallstreetbets have an impact on the stock market and how some companies are \n",
    "discussed more due to the relevance/popularity of the company in the general space.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import yfinance as yf\n",
    "import openai\n",
    "from os import getenv\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import datetime\n",
    "import time\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### GET THE TOP 13 SELECTED STOCKS #########\n",
    "\n",
    "symbols = pd.read_csv('symbols/symbols_top_20.csv')\n",
    "symbols.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### REMOVE DUPLICATES ####### \n",
    "\n",
    "def remove_duplicates(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, 'r') as f:\n",
    "            sentences = f.readlines()\n",
    "\n",
    "        # Remove duplicates while preserving the order\n",
    "        unique_sentences = list(dict.fromkeys(sentences))\n",
    "\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.writelines(unique_sentences)\n",
    "\n",
    "        print(\"Duplicates removed successfully and saved to\", output_file)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found. Please check the input file path.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot files\n",
    "input_file = 'raw_txt/hot.txt'\n",
    "output_file = 'clean_txt/clean_hot.txt'\n",
    "\n",
    "remove_duplicates(input_file, output_file)\n",
    "\n",
    "# New file\n",
    "input_file = 'raw_txt/new.txt'\n",
    "output_file = 'clean_txt/clean_new.txt'\n",
    "\n",
    "remove_duplicates(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### CONVERT THE LINES TO A DATAFRAME #############\n",
    "def make_dataframe(file_name):\n",
    "    rows = []\n",
    "\n",
    "    with open(file_name, 'r') as file:\n",
    "        current_date = None\n",
    "        for line in file:\n",
    "            # Check if the line matches the date format\n",
    "            match = re.search(r'[-]+(\\d{4}-\\d{2}-\\d{2})', line)\n",
    "            if match:\n",
    "                # Extract the date in the format \"YYYY-MM-DD\"\n",
    "                current_date = match.group(1) # Only extract the date part without dashes\n",
    "            else:\n",
    "                if current_date is not None: # Avoid adding rows without an associated date\n",
    "                    rows.append({'date': current_date, 'text': line.strip()})\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=['date', 'text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_df = make_dataframe('clean_txt/clean_hot.txt')\n",
    "new_df = make_dataframe('clean_txt/clean_new.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write variables into a csv file \n",
    "hot_df.to_csv('text_df/hot_df.csv', index=False)\n",
    "new_df.to_csv('text_df/new_df.csv', index=False)\n",
    "\n",
    "#read the df into the variable\n",
    "hot_df = pd.read_csv('text_df/hot_df.csv')\n",
    "new_df = pd.read_csv('text_df/new_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### FIND TICKERS IN THE TEXT ##########\n",
    "\n",
    "def symbol_matches(sentence, symbol_list):\n",
    "    matches = []\n",
    "    for symbol in symbol_list:\n",
    "        if pd.notna(symbol): \n",
    "            pattern = r\"\\b\" + re.escape(str(symbol)) + r\"\\b\"\n",
    "            if re.search(pattern, sentence):\n",
    "                matches.append(symbol)\n",
    "    return matches\n",
    "\n",
    "def find_matches(hot_df, symbols):\n",
    "    results = []\n",
    "    for index, row in hot_df.iterrows():\n",
    "        date = row['date']\n",
    "        sentence = row['text']\n",
    "        try:\n",
    "            # Find matches for each sentence with \"Symbol\" and \"Dollar_Symbol\" columns\n",
    "            matches_symbols = symbol_matches(sentence, symbols['Symbol'].tolist())\n",
    "            matches_dollar_symbols = symbol_matches(sentence, symbols['Dollar_Symbol'].tolist())\n",
    "            all_matches = matches_symbols + matches_dollar_symbols\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(sentence)\n",
    "            break;\n",
    "        # If no match is found then input \"S&P\"\n",
    "        if not all_matches:\n",
    "            all_matches = ['S&P']\n",
    "\n",
    "        ticker_string = ', '.join(all_matches)\n",
    "        results.append((date, sentence, ticker_string))\n",
    "    tickers = pd.DataFrame(results, columns=[\"date\", \"text\", \"ticker\"])\n",
    "    return tickers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_ticker = find_matches(hot_df,symbols)\n",
    "new_ticker = find_matches(new_df,symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write variables to a csv with no index\n",
    "hot_ticker.to_csv('text_df/hot_ticker.csv', index=False)\n",
    "new_ticker.to_csv('text_df/new_ticker.csv', index=False)\n",
    "\n",
    "#read the csv files\n",
    "hot_ticker = pd.read_csv('text_df/hot_ticker.csv')\n",
    "new_ticker = pd.read_csv('text_df/new_ticker.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FINBERT #####\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe_finbert = pipeline(\"text-classification\", model=\"ProsusAI/finbert\")\n",
    "\n",
    "def finbert_sentiments(df,pipe=pipe_finbert):\n",
    "    rows = []\n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "\n",
    "        if isinstance(text, str):\n",
    "            sentences = [text]\n",
    "        #send to pipeline\n",
    "        results = pipe(sentences)\n",
    "\n",
    "        for sentence, result in zip(sentences, results):\n",
    "            rows.append((date, sentence, ticker, result['label'], result['score']))\n",
    "\n",
    "    finbert_sentiment = pd.DataFrame(rows, columns=['date', 'text', 'ticker', 'label', 'score'])\n",
    "    \n",
    "    return finbert_sentiment\n",
    "\n",
    "hot_finbert = finbert_sentiments(hot_ticker['text'].str.lower())\n",
    "new_finbert = finbert_sentiments(new_ticker['text'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write variable into a csv with no index\n",
    "hot_finbert.to_csv('sentiment_scores/hot_finbert.csv', index=False)\n",
    "new_finbert.to_csv('sentiment_scores/new_finbert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### VADER #######\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer_vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiments(df,analyzer=analyzer_vader):\n",
    "    rows = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "        if isinstance(text, str):\n",
    "            sentences = [text]\n",
    "        #send to pipeline\n",
    "        for sentence in sentences:\n",
    "            result = analyzer.polarity_scores(sentence)\n",
    "            rows.append((date, sentence, ticker, result['neg'], result['neu'], result['pos'], result['compound']))\n",
    "\n",
    "    vader_sentiment = pd.DataFrame(rows, columns=['date', 'text', 'ticker', 'neg', 'neu', 'pos', 'compound'])\n",
    "    \n",
    "    return vader_sentiment\n",
    "\n",
    "    hot_vader = vader_sentiments(hot_ticker)\n",
    "    new_vader = vader_sentiments(new_ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the variable to csv file with no index\n",
    "hot_vader.to_csv('sentiment_scores/hot_vader.csv', index=False)\n",
    "new_vader.to_csv('sentiment_scores/new_vader.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## ROBERTA #############\n",
    "\n",
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "pipe_roberta = pipeline(\"text-classification\", model=\"mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis\")\n",
    "\n",
    "def roberta_sentiments(df,pipe=pipe_roberta):\n",
    "    rows = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "        if isinstance(text, str):\n",
    "            sentences = [text]\n",
    "        results = pipe(sentences)\n",
    "        \n",
    "        for sentence, result in zip(sentences, results):\n",
    "            rows.append((date, sentence, ticker, result['label'], result['score']))\n",
    "\n",
    "    roberta_sentiment = pd.DataFrame(rows, columns=['date', 'text', 'ticker', 'label', 'score'])\n",
    "    return roberta_sentiment\n",
    "\n",
    "\n",
    "hot_roberta = roberta_sentiments(hot_ticker['text'].str.lower())\n",
    "new_roberta = roberta_sentiments(new_ticker['text'].str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write sentiments into a csv with no index\n",
    "hot_roberta.to_csv('sentiment_scores/hot_roberta.csv', index=False)\n",
    "new_roberta.to_csv('sentiment_scores/new_roberta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## GPT-3 ########\n",
    "load_dotenv()\n",
    "openai.api_key = getenv('OPENAI_API_KEY')\n",
    "\n",
    "tmp_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(content):\n",
    "    return openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Analyze the given text and classify it into: negative, or positive. Also provide a sentiment score within the range of -1 to 1. Score values must be calculated with high precision with up to three decimal places. Your response format should be: sentiment, score e.g., ('negative, -0.145').\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": content\n",
    "            }\n",
    "        ],\n",
    "        temperature=1,\n",
    "        max_tokens=256,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "def chatgpt_df(rows):\n",
    "    chatgpt = pd.DataFrame(rows, columns=['date', 'text', 'ticker', 'label', 'score'])\n",
    "    return chatgpt\n",
    "\n",
    "def chatgpt_sentiments(df, tmp_df=tmp_df, max_retries=5, base_delay=1, analyzer=openai):\n",
    "    rows = []\n",
    "    total_tokens = 0\n",
    "    calls_per_minute = 1000\n",
    "    call_interval = 60 / calls_per_minute\n",
    "    last_call_time = 0\n",
    "    last_token_reset_time = time.time() # Keep track of when the token count was last reset\n",
    "    max_loop_count = 10 * max_retries # Maximum number of consecutive loop iterations without progress\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "        retries = 0\n",
    "        loop_count = 0 # Counter for loop iterations without progress\n",
    "        \n",
    "        while retries < max_retries:\n",
    "            current_time = time.time()\n",
    "            time_since_last_call = current_time - last_call_time\n",
    "            time_since_last_token_reset = current_time - last_token_reset_time\n",
    "\n",
    "            if time_since_last_token_reset >= 60:\n",
    "                total_tokens = 0 # Reset token count if it's been more than a minute\n",
    "                last_token_reset_time = current_time\n",
    "\n",
    "            if time_since_last_call < call_interval or total_tokens >= 85000:\n",
    "                sleep_time = call_interval - time_since_last_call\n",
    "                print(f\"Rate limit reached, sleeping for {sleep_time} seconds...\")\n",
    "                time.sleep(sleep_time)\n",
    "                loop_count += 1\n",
    "                if loop_count > max_retries:  # Break out of the loop if too many iterations have occurred\n",
    "                    print(\"Max loop count reached, unable to process the request.\")\n",
    "                    break\n",
    "                continue \n",
    "                \n",
    "                loop_count = 0\n",
    "            try:\n",
    "                last_call_time = time.time() # Update the time of the last successful call\n",
    "                result = get_response(text)\n",
    "                tokens = result['usage']['total_tokens']\n",
    "                total_tokens += tokens # Update the total token count\n",
    "                content = result.choices[0].message['content']\n",
    "                try:\n",
    "                    label, score_str = content.split(',', 1) # Split only at the first comma\n",
    "                    label = label.strip()\n",
    "                    score = float(score_str.strip())\n",
    "                except ValueError:\n",
    "                    print(f\"Error here: {content}\")\n",
    "                    label = \"error\"\n",
    "                    score = 2\n",
    "                \n",
    "                rows.append((date, text, ticker, label, score))\n",
    "                print(f\"Sentiment captured: {score}, token_used: {tokens}, total for the min: {total_tokens}\")\n",
    "                break\n",
    "\n",
    "            except openai.error.ServiceUnavailableError:\n",
    "                tmp_df = chatgpt_df(rows) #save the answers till the error point.\n",
    "                tmp_df.to_csv('tmp.csv', index=False) #write tmp_df to a csv file named tmp with no index\n",
    "                print(\"Did not get result.\")\n",
    "                delay = base_delay * (2 ** retries)\n",
    "                print(f\"Service unavailable, retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                retries += 1\n",
    "            except Exception as e:  # Catch-all for other exceptions\n",
    "                tmp_df = chatgpt_df(rows) #save the answers till the error point.\n",
    "                tmp_df.to_csv('tmp.csv', index=False)\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "                delay = base_delay * (2 ** retries)\n",
    "                print(f\"Retrying in {delay} seconds...\")\n",
    "                time.sleep(delay)\n",
    "                retries += 1\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"Operation was interrupted by the user.\")\n",
    "                tmp_df = chatgpt_df(rows) #save the answers till the error point.\n",
    "                tmp_df.to_csv('tmp.csv', index=False)\n",
    "                return chatgpt_df(rows)\n",
    "        else: # This will execute if the while loop ends without a break statement\n",
    "            print(\"Max retries reached, unable to process the request.\")\n",
    "\n",
    "    return chatgpt_df(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_chatgpt = chatgpt_sentiments(hot_ticker)\n",
    "new_chatgpt = chatgpt_sentiments(new_ticker)\n",
    "\n",
    "#retry the rows which have an error\n",
    "hot_tmp = chatgpt_sentiments(hot_chatgpt[hot_chatgpt['score']==2])\n",
    "# Iterate through the merged DataFrame and update the label and score in chatgpt\n",
    "hot_merged_df = pd.merge(hot_chatgpt, hot_tmp, on='text', suffixes=('_hot', '_tmp'))\n",
    "for index, row in hot_merged_df.iterrows():\n",
    "    hot_index = hot_chatgpt[hot_chatgpt['text'] == row['text']].index.item()\n",
    "    hot_chatgpt.at[hot_index, 'label'] = row['label_tmp']\n",
    "    hot_chatgpt.at[hot_index, 'score'] = row['score_tmp']\n",
    "\n",
    "new_tmp = chatgpt_sentiments(new_chatgpt[new_chatgpt['score']==2])\n",
    "new_merged_df = pd.merge(new_chatgpt, new_tmp, on='text', suffixes=('_new', '_tmp'))\n",
    "for index, row in new_merged_df.iterrows():\n",
    "    new_index = new_chatgpt[new_chatgpt['text'] == row['text']].index.item()\n",
    "    new_chatgpt.at[new_index, 'label'] = row['label_tmp']\n",
    "    new_chatgpt.at[new_index, 'score'] = row['score_tmp']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the label \n",
    "hot_chatgpt['label'] = hot_chatgpt['label'].replace({\n",
    "    'Positive': 'positive',\n",
    "    'Neutral': 'neutral',\n",
    "    'Negative': 'negative',\n",
    "    'No sentiment': 'neutral',\n",
    "    'neither': 'neutral'\n",
    "})\n",
    "\n",
    "new_chatgpt['label'] = new_chatgpt['label'].replace({\n",
    "    'Positive': 'positive',\n",
    "    'Neutral': 'neutral',\n",
    "    'Negative': 'negative',\n",
    "    'No sentiment': 'neutral',\n",
    "    'neither': 'neutral'\n",
    "})\n",
    "\n",
    "#remove the rows from the df where the label is 'error'\n",
    "hot_chatgpt = hot_chatgpt[hot_chatgpt['label'] != 'error']\n",
    "new_chatgpt = new_chatgpt[new_chatgpt['label'] != 'error']\n",
    "\n",
    "#remove the rows from the df where the label is 'error'\n",
    "hot_chatgpt = hot_chatgpt[hot_chatgpt['label'] != 'error']\n",
    "new_chatgpt = new_chatgpt[new_chatgpt['label'] != 'error']\n",
    "\n",
    "# write the df to csv file with no index\n",
    "hot_chatgpt.to_csv('sentiment_scores/hot_chatgpt.csv', index=False)\n",
    "new_chatgpt.to_csv('sentiment_scores/new_chatgpt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the hot_chatgpt variable\n",
    "hot_chatgpt = pd.read_csv('sentiment_scores/hot_chatgpt.csv')\n",
    "new_chatgpt = pd.read_csv('sentiment_scores/new_chatgpt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## DATA ANALYSIS #########\n",
    "\n",
    "hot_finbert = pd.read_csv('sentiment_scores/hot_finbert.csv')\n",
    "hot_vader = pd.read_csv('sentiment_scores/hot_vader.csv')\n",
    "hot_chatgpt = pd.read_csv('sentiment_scores/hot_chatgpt.csv')\n",
    "hot_roberta = pd.read_csv('sentiment_scores/hot_roberta.csv')\n",
    "new_finbert = pd.read_csv('sentiment_scores/new_finbert.csv')\n",
    "new_vader = pd.read_csv('sentiment_scores/new_vader.csv')\n",
    "new_chatgpt = pd.read_csv('sentiment_scores/new_chatgpt.csv')\n",
    "new_roberta = pd.read_csv('sentiment_scores/new_roberta.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def correlation_matrix(hot_sentiment, new_sentiment):\n",
    "\n",
    "    sentiment_data = pd.concat([hot_sentiment, new_sentiment], ignore_index=True)\n",
    "    sentiment_data.drop_duplicates(subset=['text'], inplace=True)\n",
    "    # Using CountVectorizer to create a word matrix and removing stop words\n",
    "    vectorizer = CountVectorizer(stop_words='english', max_features=10)\n",
    "    word_matrix_vectorized = vectorizer.fit_transform(sentiment_data['text'])\n",
    "\n",
    "    # Calculating the correlation matrix using CountVectorizer\n",
    "    correlation_matrix_vectorized = np.corrcoef(word_matrix_vectorized.toarray(), rowvar=False)\n",
    "\n",
    "    # Plotting the correlation matrix using CountVectorizer\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix_vectorized, annot=True, cmap='coolwarm', xticklabels=vectorizer.get_feature_names_out(), yticklabels=vectorizer.get_feature_names_out())\n",
    "    plt.title('Correlation Matrix of Top 10 Words')\n",
    "    plt.show()\n",
    "\n",
    "# Words are same in all models, so running it once is good\n",
    "correlation_matrix(hot_finbert, new_finbert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis(hot_sentiment, new_sentiment, model):\n",
    "    print(f\" ========= Analysis of the sentiment scores derived from {model} ========= \\n\\n\")\n",
    "    sentiment_data = pd.concat([hot_sentiment, new_sentiment], ignore_index=True)\n",
    "    sentiment_data.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "    sentiment_counts = sentiment_data['label'].value_counts(normalize=True) * 100\n",
    "\n",
    "    color_map = {\n",
    "        'neutral': 'lightblue',\n",
    "        'negative': 'red',\n",
    "        'positive': 'green'\n",
    "    }    \n",
    "    # Plotting the overall sentiment distribution\n",
    "    pie_colors = [color_map[label] for label in sentiment_counts.index]\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sentiment_counts.plot.pie(autopct='%1.1f%%',colors=pie_colors)\n",
    "    plt.title(f'Overall Sentiment Distribution - {model}')\n",
    "    plt.show()\n",
    "\n",
    "    # Accesing the top 5 tickers labels\n",
    "    top_6 = sentiment_data['ticker'].value_counts().head(6)\n",
    "    top_5_no_sp = top_6[1:6]\n",
    "\n",
    "    top_5_ticker_analysis = {}\n",
    "    for ticker in top_5_no_sp.index:\n",
    "        top_5_ticker_analysis[ticker] = (sentiment_data[sentiment_data['ticker'] == ticker]['label'].value_counts(normalize=True) * 100).to_dict()\n",
    "\n",
    "    top_5_ticker_df = pd.DataFrame.from_dict(top_5_ticker_analysis, orient='index').fillna(0)\n",
    "    print(top_5_ticker_df.head())\n",
    "\n",
    "    # Plotting the sentiment distribution for the top 5 tickers\n",
    "    bar_colors = [color_map[label] for label in top_5_ticker_df.columns]\n",
    "    top_5_ticker_df.plot(kind='bar', stacked=True, figsize=(10, 6), color=bar_colors)\n",
    "    plt.title(f'Sentiment Distribution for Top 5 Tickers - {model} ')\n",
    "    plt.xlabel('Ticker')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    #Plotting the distribution of the sentiment scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(sentiment_data['score'], kde=True, bins=20, color='purple')\n",
    "    plt.title(f'Distribution of Sentiment Scores - {model}')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "model_analysis(hot_finbert, new_finbert, 'Finbert')\n",
    "model_analysis(hot_roberta, new_roberta, 'RoBERTa')\n",
    "model_analysis(hot_chatgpt, new_chatgpt, 'GPT-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_analysis_vader(hot_sentiment, new_sentiment):\n",
    "\n",
    "    print(f\" ========= Analysis of the sentiment scores derived from VADER ========= \\n\\n\")\n",
    "    print(\"\")\n",
    "    sentiment_data = pd.concat([hot_sentiment, new_sentiment], ignore_index=True)\n",
    "    sentiment_data.drop_duplicates(subset=['text'], inplace=True)\n",
    "\n",
    "    #Plotting the distribution of the sentiment scores\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(sentiment_data['compound'], kde=True, bins=20, color='purple')\n",
    "    plt.title(f'Distribution of Sentiment Scores - VADER')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "model_analysis_vader(hot_vader, new_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### DOWNLOAD STOCK TICKERS #########\n",
    "\n",
    "#test S&P 500 since the symbol in yf is unique\n",
    "sp = download_stock_data(\"^GSPC\")\n",
    "sp.head()\n",
    "#write it to a csv file\n",
    "sp.to_csv('price_data/top_13/S&P.csv')\n",
    "\n",
    "def download_stock_data(ticker):\n",
    "    start_date = pd.to_datetime('2022-08-02')\n",
    "    end_date = pd.to_datetime('2023-08-17')\n",
    "    #download the stock\n",
    "    data = yf.download(ticker, start=start_date, end=end_date)\n",
    "    return data\n",
    "\n",
    "def download_ticker(ticker_list):\n",
    "    for ticker in ticker_list:\n",
    "        print(ticker)\n",
    "        # Call the download_stock_data function to get adjusted closing prices\n",
    "        stock_data = download_stock_data(ticker)\n",
    "        #write the stock data to a csv file in the price_data folder\n",
    "        stock_data.to_csv('price_data/top_13/'+ticker+'.csv')\n",
    "\n",
    "#convert the tickers to a list\n",
    "ticker_list = list(set(hot_ticker['ticker'].tolist()))\n",
    "\n",
    "download_ticker(ticker_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### READ THE DFs INTO A DICT #######\n",
    "\n",
    "\n",
    "price_data = {}\n",
    "for ticker in ticker_list:\n",
    "    file_path = os.path.join('price_data','top_13' ,f'{ticker}.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        price_data[ticker] = pd.read_csv(file_path)\n",
    "    else:\n",
    "        print(f\"File for {ticker} does not exist!\")\n",
    "\n",
    "\n",
    "####### CALCULATE THE RETURNS #######\n",
    "\n",
    "\n",
    "for ticker, df in price_data.items():\n",
    "    close_prices = df['Close']\n",
    "    for days in range(1, 6):\n",
    "        df[f'Returns_{days}'] = close_prices.pct_change(periods=days)\n",
    "    price_data[ticker] = df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## MERGE THE RETURNS AND SENTIMENT SCORES ##############\n",
    "\n",
    "def merge_dataframes(sentiment_df, price_data_dict):\n",
    "    merged_rows = []\n",
    "    for index, row in sentiment_df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "        label = row['label']\n",
    "        score = row['score']\n",
    "\n",
    "        # Check if the ticker exists in price_data_ret\n",
    "        if ticker in price_data_dict:\n",
    "            ticker_df = price_data_dict[ticker]\n",
    "        else:\n",
    "            # If ticker not found, use the 'S&P' DataFrame\n",
    "            ticker_df = price_data_dict['S&P']\n",
    "            ticker = 'S&P'\n",
    "        \n",
    "        matching_date = ticker_df[ticker_df['Date'] == date]\n",
    "\n",
    "        \n",
    "\n",
    "        # If a matching date is found, merge the entire row\n",
    "        if not matching_date.empty:\n",
    "            op = matching_date['Open'].iloc[0]\n",
    "            hi = matching_date['High'].iloc[0]\n",
    "            lo = matching_date['Low'].iloc[0]\n",
    "            cl = matching_date['Close'].iloc[0]\n",
    "            vol = matching_date['Volume'].iloc[0]\n",
    "            ret_1 = matching_date['Returns_1'].iloc[0]\n",
    "            ret_2 = matching_date['Returns_2'].iloc[0]\n",
    "            ret_3 = matching_date['Returns_3'].iloc[0]\n",
    "            ret_4 = matching_date['Returns_4'].iloc[0]\n",
    "            ret_5 = matching_date['Returns_5'].iloc[0]\n",
    "\n",
    "            merged_rows.append([date, text, ticker,label,score, op, hi, lo, cl, vol, ret_1, ret_2, ret_3, ret_4, ret_5])\n",
    "\n",
    "    #make new df\n",
    "    merged_df = pd.DataFrame(merged_rows, columns=['date','text','ticker','label', 'score', 'open','high','low','close','vol','ret_1','ret_2','ret_3','ret_4','ret_5'])\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "# combine the sentiment scores with the price data\n",
    "hot_finbert_ret = merge_dataframes(hot_finbert,price_data)\n",
    "new_finbert_ret = merge_dataframes(new_finbert,price_data)\n",
    "\n",
    "hot_chatgpt_ret = merge_dataframes(hot_chatgpt,price_data)\n",
    "new_chatgpt_ret = merge_dataframes(new_chatgpt,price_data)\n",
    "\n",
    "hot_roberta_ret = merge_dataframes(hot_roberta,price_data)\n",
    "new_roberta_ret = merge_dataframes(new_roberta,price_data)\n",
    "\n",
    "#write the variables to save the csv\n",
    "hot_finbert_ret.to_csv('sentiment_score_ret/top_13/hot_finbert_ret.csv',index=False)\n",
    "new_finbert_ret.to_csv('sentiment_score_ret/top_13/new_finbert_ret.csv',index=False)\n",
    "hot_chatgpt_ret.to_csv('sentiment_score_ret/top_13/hot_chatgpt_ret.csv',index=False)\n",
    "new_chatgpt_ret.to_csv('sentiment_score_ret/top_13/new_chatgpt_ret.csv',index=False)\n",
    "hot_roberta_ret.to_csv('sentiment_score_ret/top_13/hot_roberta_ret.csv',index=False)\n",
    "new_roberta_ret .to_csv('sentiment_score_ret/top_13/new_roberta_ret.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER\n",
    "\n",
    "def merge_dataframes_vader(sentiment_df, price_data_dict):\n",
    "    # List to store the merged rows\n",
    "    merged_rows = []\n",
    "\n",
    "    # Iterate through the rows in sentiment_df\n",
    "    for index, row in sentiment_df.iterrows():\n",
    "        date = row['date']\n",
    "        text = row['text']\n",
    "        ticker = row['ticker']\n",
    "        compound = row['compound']\n",
    "\n",
    "        # Check if the ticker exists in price_data_ret\n",
    "        if ticker in price_data_dict:\n",
    "            ticker_df = price_data_dict[ticker]\n",
    "        else:\n",
    "            # If ticker not found, use the 'S&P' DataFrame\n",
    "            ticker_df = price_data_dict['S&P']\n",
    "            ticker = 'S&P'\n",
    "        \n",
    "        matching_date = ticker_df[ticker_df['Date'] == date]\n",
    "\n",
    "        \n",
    "\n",
    "        # If a matching date is found, merge the entire row\n",
    "        if not matching_date.empty:\n",
    "            op = matching_date['Open'].iloc[0]\n",
    "            hi = matching_date['High'].iloc[0]\n",
    "            lo = matching_date['Low'].iloc[0]\n",
    "            cl = matching_date['Close'].iloc[0]\n",
    "            vol = matching_date['Volume'].iloc[0]\n",
    "            ret_1 = matching_date['Returns_1'].iloc[0]\n",
    "            ret_2 = matching_date['Returns_2'].iloc[0]\n",
    "            ret_3 = matching_date['Returns_3'].iloc[0]\n",
    "            ret_4 = matching_date['Returns_4'].iloc[0]\n",
    "            ret_5 = matching_date['Returns_5'].iloc[0]\n",
    "\n",
    "            merged_rows.append([date, text, ticker,compound, op, hi, lo, cl, vol, ret_1, ret_2, ret_3, ret_4, ret_5])\n",
    "\n",
    "\n",
    "    # Concatenate all the merged rows to create the merged DataFrame\n",
    "    merged_df = pd.DataFrame(merged_rows, columns=['date','text','ticker','compound', 'open','high','low','close','vol','ret_1','ret_2','ret_3','ret_4','ret_5'])\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "hot_vader_top = merge_dataframes_vader(hot_vader,price_data_top_13)\n",
    "new_vader_top = merge_dataframes_vader(new_vader,price_data_top_13)\n",
    "\n",
    "#write vader to csv file\n",
    "hot_vader_top.to_csv('sentiment_score_ret/top_13/hot_vader_ret.csv',index=False)\n",
    "new_vader_top.to_csv('sentiment_score_ret/top_13/new_vader_ret.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# NEURAL NETWORK MODELS ##############\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, Conv1D, MaxPooling1D, GlobalMaxPooling1D, concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model,pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import Model, Input, layers, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_and_accuracy(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    axs[0].plot(history.history['accuracy'])\n",
    "    axs[0].plot(history.history['val_accuracy'])\n",
    "    axs[0].set_title('Model accuracy')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    axs[1].plot(history.history['loss'])\n",
    "    axs[1].plot(history.history['val_loss'])\n",
    "    axs[1].set_title('Model loss')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_week_eval(accuracy,precision,recall,loss):\n",
    "  labels = ['Day 1', 'Day 2', 'Day 3', 'Day 4', 'Day 5']\n",
    "  fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "  # Accuracy\n",
    "  axes[0, 0].plot(labels, accuracy, marker='o')\n",
    "  axes[0, 0].set_title('Accuracy')\n",
    "  axes[0, 0].grid(True)\n",
    "  # Precision\n",
    "  axes[0, 1].plot(labels, precision, marker='o')\n",
    "  axes[0, 1].set_title('Precision')\n",
    "  axes[0, 1].grid(True)\n",
    "  #Recall\n",
    "  axes[1, 0].plot(labels, recall, marker='o')\n",
    "  axes[1, 0].set_title('Recall')\n",
    "  axes[1, 0].grid(True)\n",
    "  #Loss\n",
    "  axes[1, 1].plot(labels, loss, marker='o')\n",
    "  axes[1, 1].set_title('Loss')\n",
    "  axes[1, 1].grid(True)\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### LSTM #########\n",
    "\n",
    "\n",
    "def LSTM_weekly(df, return_day, max_words=10000):\n",
    "    # Preprocessing\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    label_dummies = pd.get_dummies(df['label'], prefix='label')\n",
    "    ticker_dummies = pd.get_dummies(df['ticker'], prefix='ticker')\n",
    "    df = pd.concat([df, label_dummies, ticker_dummies], axis=1)\n",
    "    df['score'] = df['score'].astype(float)\n",
    "\n",
    "    X = df[['text'] + list(label_dummies.columns) + list(ticker_dummies.columns) + ['score', 'open', 'high', 'low', 'close', 'vol']]\n",
    "    y = (df[f'ret_{return_day}'] > 0).astype(int)\n",
    "\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Text Tokenization and Padding\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train['text'])\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train['text'])\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test['text'])\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Model Inputs\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    ticker_input = Input(shape=(len(ticker_dummies.columns),), name='ticker_input')\n",
    "    label_input = Input(shape=(len(label_dummies.columns),), name='label_input')\n",
    "    score_input = Input(shape=(1,), name='score_input')\n",
    "    market_input = Input(shape=(5, 1), name='market_input')  # Changed shape to 3D for LSTM\n",
    "\n",
    "    # Model Layers\n",
    "    text_embedding = layers.Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_lstm = layers.LSTM(128)(text_embedding)\n",
    "    text_dropout = Dropout(0.5)(text_lstm)\n",
    "\n",
    "    # LSTM layer for market data\n",
    "    market_lstm = layers.LSTM(32)(market_input) \n",
    "    market_dropout = Dropout(0.5)(market_lstm)\n",
    "\n",
    "    # Concatenate all inputs\n",
    "    combined_inputs = layers.concatenate([text_dropout, ticker_input, label_input, score_input, market_dropout])\n",
    "\n",
    "    # LSTM layer\n",
    "    x = layers.Reshape((combined_inputs.shape[1], 1))(combined_inputs)\n",
    "    x = layers.LSTM(128)(x)\n",
    "    \n",
    "    # Dense layers\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Dense(68, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=[text_input, ticker_input, label_input, score_input, market_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', metrics.Precision(name='precision'), metrics.Recall(name='recall')])\n",
    "\n",
    "    #Plot the model only one time\n",
    "    if return_day == 1:\n",
    "        tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "        model.summary()\n",
    "        \n",
    "    #Train the model   \n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train[ticker_dummies.columns].values, \n",
    "         'label_input': X_train[label_dummies.columns].values, 'score_input': X_train['score'].values,\n",
    "         'market_input': X_train[['open', 'high', 'low', 'close', 'vol']].values.reshape(-1, 5, 1)},  # Reshaped for LSTM\n",
    "        y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test[ticker_dummies.columns].values, \n",
    "         'label_input': X_test[label_dummies.columns].values, 'score_input': X_test['score'].values,\n",
    "         'market_input': X_test[['open', 'high', 'low', 'close', 'vol']].values.reshape(-1, 5, 1)},  # Reshaped for LSTM\n",
    "        y_test\n",
    "    )\n",
    "    print(\"Test Accuracy for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[1] * 100))\n",
    "    print(\"Test Precision for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[2] * 100))\n",
    "    print(\"Test Recall for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[3] * 100))\n",
    "    print(\"Test Loss for Returns from day \", return_day, \" : {:.4f}\".format(evaluation[0]))\n",
    "\n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT FINBERT LSTM  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = LSTM_weekly(hot_finbert,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2]) \n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)\n",
    "\n",
    "print(\"=============  NEW FINBERT LSTM  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = LSTM_weekly(new_finbert,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2]) \n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT ROBERTA LSTM  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = LSTM_weekly(hot_roberta,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2]) \n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)\n",
    "\n",
    "print(\"=============  NEW ROBERTA LSTM  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = LSTM_weekly(new_roberta,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2]) \n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT GPT-3 LSTM  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = LSTM_weekly(hot_chatgpt,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2]) \n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)\n",
    "\n",
    "print(\"=============  NEW GPT-3 LSTM  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = LSTM_weekly(new_chatgpt,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2]) \n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VADER\n",
    "\n",
    "def LSTM_weekly_vader(df, return_day, max_words=10000):\n",
    "    # Preprocessing\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    ticker_dummies = pd.get_dummies(df['ticker'], prefix='ticker')\n",
    "    df = pd.concat([df, ticker_dummies], axis=1)\n",
    "    \n",
    "    # Feature and Target Variables\n",
    "    X = df[['text'] + list(ticker_dummies.columns) + ['compound', 'open', 'high', 'low', 'close', 'vol']]\n",
    "    y = (df[f'ret_{return_day}'] > 0).astype(int)\n",
    "\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Text Tokenization and Padding\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train['text'])\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train['text'])\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test['text'])\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Model Inputs\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    ticker_input = Input(shape=(len(ticker_dummies.columns),), name='ticker_input')\n",
    "    compound_input = Input(shape=(1,), name='compound_input')\n",
    "    market_input = Input(shape=(5,), name='market_input')\n",
    "\n",
    "    # Model Layers\n",
    "    text_embedding = layers.Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_lstm = layers.LSTM(128)(text_embedding)\n",
    "    text_dropout = Dropout(0.5)(text_lstm)\n",
    "    \n",
    "    market_dense = layers.Dense(32, activation='relu', kernel_regularizer=l2(0.001))(market_input)\n",
    "    market_dropout = Dropout(0.5)(market_dense)\n",
    "\n",
    "    combined_inputs = layers.concatenate([text_dropout, ticker_input, compound_input, market_dropout])\n",
    "    \n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001))(combined_inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Dense(68, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Compile and Train Model\n",
    "    model = Model(inputs=[text_input, ticker_input, compound_input, market_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', metrics.Precision(name='precision'), metrics.Recall(name='recall')])\n",
    "    \n",
    "    if return_day == 1:\n",
    "        tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "        model.summary()\n",
    "        \n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train[ticker_dummies.columns].values, \n",
    "         'compound_input': X_train['compound'].values,\n",
    "         'market_input': X_train[['open', 'high', 'low', 'close', 'vol']].values},\n",
    "        y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    plot_loss_and_accuracy(history)\n",
    "    # Evaluate the model\n",
    "    evaluation = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test[ticker_dummies.columns].values, \n",
    "         'compound_input': X_test['compound'].values,\n",
    "         'market_input': X_test[['open', 'high', 'low', 'close', 'vol']].values},\n",
    "        y_test\n",
    "    )\n",
    "\n",
    "    print(\"Test Accuracy for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[1] * 100))\n",
    "    print(\"Test Precision for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[2] * 100))\n",
    "    print(\"Test Recall for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[3] * 100))\n",
    "    print(\"Test Loss for Returns from day \", return_day, \" : {:.4f}\".format(evaluation[0]))\n",
    "\n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT VADER LSTM  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = LSTM_weekly_vader(hot_vader,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2]) \n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)\n",
    "\n",
    "print(\"=============  NEW VADER LSTM  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = LSTM_weekly_vader(new_vader,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2]) \n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ CNN #############\n",
    "\n",
    "def CNN_weekly(df, return_day, max_words=10000):\n",
    "\n",
    "    #Preprocessing\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    df['score'] = df['score'].astype(float)\n",
    "    label_dummies = pd.get_dummies(df['label'], prefix='label')\n",
    "    ticker_dummies = pd.get_dummies(df['ticker'], prefix='ticker')\n",
    "    df = pd.concat([df, label_dummies, ticker_dummies], axis=1)\n",
    "    \n",
    "    X = df[['text'] + list(label_dummies.columns) + list(ticker_dummies.columns) + ['score', 'open', 'high', 'low', 'close', 'vol']]\n",
    "    y = (df[f'ret_{return_day}'] > 0).astype(int)\n",
    "\n",
    "    # Split the data into a training set and a testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #Text Tokenization and Padding\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train['text'])\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train['text'])\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test['text'])\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Model Inputs\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    ticker_input = Input(shape=(len(ticker_dummies.columns),), name='ticker_input')\n",
    "    label_input = Input(shape=(len(label_dummies.columns),), name='label_input')\n",
    "    score_input = Input(shape=(1,), name='score_input')\n",
    "    market_input = Input(shape=(5, 1), name='market_input')\n",
    "\n",
    "    #Convert to Embedding and define the text layers\n",
    "    text_embedding = layers.Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_conv = Conv1D(128, 3, activation='relu')(text_embedding)\n",
    "    text_pool = MaxPooling1D(3)(text_conv)\n",
    "    text_flat = GlobalMaxPooling1D()(text_pool)\n",
    "    text_dropout = Dropout(0.5)(text_flat)\n",
    "    # Market layers\n",
    "    market_conv1 = Conv1D(32, 2, activation='relu')(market_input)\n",
    "    market_pool1 = MaxPooling1D(2)(market_conv1)\n",
    "    market_flat = GlobalMaxPooling1D()(market_pool1)\n",
    "    market_dropout = Dropout(0.5)(market_flat)\n",
    "\n",
    "    #Concatenate the Inputs\n",
    "    combined_inputs = layers.concatenate([text_dropout, ticker_input, label_input, score_input, market_dropout])\n",
    "\n",
    "    #Dense layers\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001))(combined_inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(68, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    #Compile the model\n",
    "    model = Model(inputs=[text_input, ticker_input, label_input, score_input, market_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', metrics.Precision(name='precision'), metrics.Recall(name='recall')])\n",
    "\n",
    "    #Plot the model only one time\n",
    "    if return_day == 1:\n",
    "        tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "        model.summary()\n",
    "\n",
    "    #Train the model\n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train[ticker_dummies.columns].values,\n",
    "         'label_input': X_train[label_dummies.columns].values, 'score_input': X_train['score'].values,\n",
    "         'market_input': X_train[['open', 'high', 'low', 'close', 'vol']].values.reshape(-1, 5, 1)},\n",
    "        y_train,\n",
    "        epochs=20,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test[ticker_dummies.columns].values,\n",
    "         'label_input': X_test[label_dummies.columns].values, 'score_input': X_test['score'].values,\n",
    "         'market_input': X_test[['open', 'high', 'low', 'close', 'vol']].values.reshape(-1, 5, 1)},\n",
    "        y_test\n",
    "    )\n",
    "    \n",
    "    print(\"Test Accuracy for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[1] * 100))\n",
    "    print(\"Test Precision for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[2] * 100))\n",
    "    print(\"Test Recall for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[3] * 100))\n",
    "    print(\"Test Loss for Returns from day \", return_day, \" : {:.4f}\".format(evaluation[0]))\n",
    "\n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT FINBERT CNN  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = CNN_weekly(hot_finbert,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2])\n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)\n",
    "\n",
    "print(\"=============  NEW FINBERT CNN  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = CNN_weekly(new_finbert,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2])\n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT ROBERTA CNN  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = CNN_weekly(hot_roberta,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2])\n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)\n",
    "\n",
    "print(\"=============  NEW ROBERTA CNN  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = CNN_weekly(new_roberta,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2])\n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT GPT-3 CNN  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = CNN_weekly(hot_chatgpt,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2])\n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)\n",
    "\n",
    "print(\"=============  NEW GPT-3 CNN  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = CNN_weekly(new_chatgpt,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2])\n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_weekly_vader(df, return_day, max_words=10000):\n",
    "    # Preprocessing\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    ticker_dummies = pd.get_dummies(df['ticker'], prefix='ticker')\n",
    "    df = pd.concat([df, ticker_dummies], axis=1)\n",
    "    df['compound'] = df['compound'].astype(float)\n",
    "\n",
    "    # Feature and Target Variables\n",
    "    X = df[['text'] + list(ticker_dummies.columns) + ['compound', 'open', 'high', 'low', 'close', 'vol']]\n",
    "    y = (df[f'ret_{return_day}'] > 0).astype(int)\n",
    "\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Text Tokenization and Padding\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(X_train['text'])\n",
    "    train_text_sequences = tokenizer.texts_to_sequences(X_train['text'])\n",
    "    test_text_sequences = tokenizer.texts_to_sequences(X_test['text'])\n",
    "    max_sequence_length = max(len(s) for s in train_text_sequences + test_text_sequences)\n",
    "    train_text_padded = pad_sequences(train_text_sequences, maxlen=max_sequence_length)\n",
    "    test_text_padded = pad_sequences(test_text_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Model Inputs\n",
    "    text_input = Input(shape=(max_sequence_length,), name='text_input')\n",
    "    ticker_input = Input(shape=(len(ticker_dummies.columns),), name='ticker_input')\n",
    "    compound_input = Input(shape=(1,), name='compound_input')\n",
    "    market_input = Input(shape=(5,), name='market_input')\n",
    "\n",
    "    # Model Layers\n",
    "    text_embedding = layers.Embedding(input_dim=max_words, output_dim=128)(text_input)\n",
    "    text_conv = Conv1D(128, 3, activation='relu')(text_embedding)\n",
    "    text_pool = MaxPooling1D(3)(text_conv)\n",
    "    text_flat = GlobalMaxPooling1D()(text_pool)\n",
    "    text_dropout = Dropout(0.5)(text_flat)\n",
    "\n",
    "    market_dense = layers.Dense(32, activation='relu', kernel_regularizer=l2(0.001))(market_input)\n",
    "    market_dropout = Dropout(0.5)(market_dense)\n",
    "\n",
    "    combined_inputs = layers.concatenate([text_dropout, ticker_input, compound_input, market_dropout])\n",
    "\n",
    "    x = layers.Dense(128, activation='relu', kernel_regularizer=l2(0.001))(combined_inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = layers.Dense(68, activation='relu', kernel_regularizer=l2(0.001))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Compile and Train Model\n",
    "    model = Model(inputs=[text_input, ticker_input, compound_input, market_input], outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', metrics.Precision(name='precision'), metrics.Recall(name='recall')])\n",
    "\n",
    "    if return_day == 1:\n",
    "        tf.keras.utils.plot_model(model, show_shapes=True)\n",
    "        model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        {'text_input': train_text_padded, 'ticker_input': X_train[ticker_dummies.columns].values,\n",
    "         'compound_input': X_train['compound'].values,\n",
    "         'market_input': X_train[['open', 'high', 'low', 'close', 'vol']].values},\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    plot_loss_and_accuracy(history)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation = model.evaluate(\n",
    "        {'text_input': test_text_padded, 'ticker_input': X_test[ticker_dummies.columns].values,\n",
    "         'compound_input': X_test['compound'].values,\n",
    "         'market_input': X_test[['open', 'high', 'low', 'close', 'vol']].values},\n",
    "        y_test\n",
    "    )\n",
    "    print(\"Test Accuracy for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[1] * 100))\n",
    "    print(\"Test Precision for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[2] * 100))\n",
    "    print(\"Test Recall for Returns from day \", return_day, \" : {:.2f}%\".format(evaluation[3] * 100))\n",
    "    print(\"Test Loss for Returns from day \", return_day, \" : {:.4f}\".format(evaluation[0]))\n",
    "\n",
    "    return evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=============  HOT VADER CNN  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = CNN_weekly_vader(hot_vader,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2])\n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)\n",
    "\n",
    "print(\"=============  NEW VADER CNN  =============== \\n\")\n",
    "accuracy =[]\n",
    "precision=[]\n",
    "recall =[]\n",
    "loss=[]\n",
    "for i in range(1,6):\n",
    "  evaluation = CNN_weekly_vader(new_vader,i)\n",
    "  loss.append(evaluation[0])\n",
    "  accuracy.append(evaluation[1])\n",
    "  precision.append(evaluation[2])\n",
    "  recall.append(evaluation[3])\n",
    "\n",
    "plot_week_eval(accuracy,precision,recall,loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stocks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
